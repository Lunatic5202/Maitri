{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-08T13:22:33.684901Z",
     "iopub.status.busy": "2025-12-08T13:22:33.684165Z",
     "iopub.status.idle": "2025-12-08T13:22:33.688981Z",
     "shell.execute_reply": "2025-12-08T13:22:33.688429Z",
     "shell.execute_reply.started": "2025-12-08T13:22:33.684878Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Importing the necessary modules\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emotions used:\n",
    "\n",
    "01 -> neutral\n",
    "\n",
    "02 -> happy\n",
    "\n",
    "03 -> sad\n",
    "\n",
    "04 -> angry\n",
    "\n",
    "05 -> disgust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T13:22:33.691099Z",
     "iopub.status.busy": "2025-12-08T13:22:33.690585Z",
     "iopub.status.idle": "2025-12-08T13:22:33.703928Z",
     "shell.execute_reply": "2025-12-08T13:22:33.703318Z",
     "shell.execute_reply.started": "2025-12-08T13:22:33.691080Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHECK 1: Does the folder exist?\n",
      "True\n",
      "\n",
      "CHECK 2: List contents of /kaggle/input/\n",
      "['ravdess-emotional-speech-audio', 'fine-tune-emotion-dataset']\n",
      "\n",
      "CHECK 3: List contents of ravdess-emotional-speech-audio folder\n",
      "['ravdess']\n",
      "\n",
      "CHECK 4: List contents of ravdess folder\n",
      "['Actor_02', 'Actor_17', 'Actor_05', 'Actor_16', 'Actor_21', 'Actor_01', 'Actor_11', 'Actor_20', 'Actor_08', 'Actor_15', 'Actor_06', 'Actor_12', 'Actor_23', 'Actor_24', 'Actor_22', 'Actor_04', 'Actor_19', 'Actor_10', 'Actor_09', 'Actor_14', 'Actor_03', 'Actor_13', 'Actor_18', 'Actor_07']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"CHECK 1: Does the folder exist?\")\n",
    "print(os.path.exists(\"/kaggle/input/ravdess-emotional-speech-audio/ravdess\"))\n",
    "\n",
    "print(\"\\nCHECK 2: List contents of /kaggle/input/\")\n",
    "print(os.listdir(\"/kaggle/input\"))\n",
    "\n",
    "print(\"\\nCHECK 3: List contents of ravdess-emotional-speech-audio folder\")\n",
    "print(os.listdir(\"/kaggle/input/ravdess-emotional-speech-audio\"))\n",
    "\n",
    "print(\"\\nCHECK 4: List contents of ravdess folder\")\n",
    "print(os.listdir(\"/kaggle/input/ravdess-emotional-speech-audio/ravdess\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T13:22:33.704756Z",
     "iopub.status.busy": "2025-12-08T13:22:33.704574Z",
     "iopub.status.idle": "2025-12-08T13:22:33.711667Z",
     "shell.execute_reply": "2025-12-08T13:22:33.711008Z",
     "shell.execute_reply.started": "2025-12-08T13:22:33.704742Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "emotion_map = {\n",
    "    \"01\": \"neutral\",\n",
    "    \"03\": \"happy\",\n",
    "    \"04\": \"sad\",\n",
    "    \"05\": \"angry\",\n",
    "    \"07\": \"disgust\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting all the emotion names as ML cannot work with words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T13:22:33.712747Z",
     "iopub.status.busy": "2025-12-08T13:22:33.712430Z",
     "iopub.status.idle": "2025-12-08T13:22:33.722319Z",
     "shell.execute_reply": "2025-12-08T13:22:33.721618Z",
     "shell.execute_reply.started": "2025-12-08T13:22:33.712718Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "classes = list(emotion_map.values())\n",
    "class_to_idx = {c: i for i, c in enumerate(classes)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Audio to Mel Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T13:22:33.724649Z",
     "iopub.status.busy": "2025-12-08T13:22:33.723558Z",
     "iopub.status.idle": "2025-12-08T13:22:33.733939Z",
     "shell.execute_reply": "2025-12-08T13:22:33.733390Z",
     "shell.execute_reply.started": "2025-12-08T13:22:33.724632Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def audio_to_mel_image(path):\n",
    "    y, sr = librosa.load(path, sr=18050) # sr indicates sampling rate, y indicates audio waveform (1D array) , Fs = 18.05 KHz.\n",
    "\n",
    "    mel = librosa.feature.melspectrogram(\n",
    "        y=y, sr=sr, n_fft=1024, hop_length=256, n_mels=128 ) # window_size = 1024.....converts raw audio into a visual time-freq plot.\n",
    "    mel_db = librosa.power_to_db(mel, ref=np.max) # convert signal to dB unit for better representation and stability.\n",
    "\n",
    "    mel_norm = (mel_db - mel_db.min()) / (mel_db.max() - mel_db.min()) # normalise the range between 0 and 1 for input simplicity.\n",
    "    mel_img = (mel_norm * 255).astype(np.uint8) # convert to pixels(0-255).\n",
    "\n",
    "    img = Image.fromarray(mel_img).convert(\"RGB\") # ResNet50 needs RGB images.\n",
    "    img = img.resize((224, 224)) # input requirement.\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old labels → used for the signature model\n",
    "\n",
    "New labels → used for the ResNet50 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T13:22:33.735464Z",
     "iopub.status.busy": "2025-12-08T13:22:33.734702Z",
     "iopub.status.idle": "2025-12-08T13:22:33.747830Z",
     "shell.execute_reply": "2025-12-08T13:22:33.747169Z",
     "shell.execute_reply.started": "2025-12-08T13:22:33.735439Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# During initialization, RAVDESSDataset scans the RAVDESS folder, picks only .wav files whose emotion code is ni target set, and stores (audio_path, label_index) pairs in self.samples.\n",
    "# When the DataLoader asks for item i, __getitem__ looks up the path and label, and then calls audio_to_mel_image(audio_path) to actually build the input for the model.\n",
    "\n",
    "    \n",
    "class RAVDESSDataset(Dataset):                                     # creating a custom PyTorch Dataset class that the DataLoader can use.\n",
    "    \n",
    "    def __init__(self, root_dir, transform=None):                  # root_dir → path to RAVDESS audio folder\n",
    "                                                                   # transform → any optional image transforms (resize, normalize, etc.)\n",
    "        \n",
    "        self.samples = []                                          # stores all (audioPath,labels) in a single list.\n",
    "        self.transform = transform\n",
    "\n",
    "        for actor in os.listdir(root_dir):                         # List all items in the main RAVDESS folder.\n",
    "                                                                   # Each \"Actor_xy\" is one folder.\n",
    "            \n",
    "            actor_folder = os.path.join(root_dir, actor)           # Create the full path......\n",
    "            if not os.path.isdir(actor_folder):                    # skip anything that is not a folder.\n",
    "                continue\n",
    "\n",
    "            for file in os.listdir(actor_folder):                  # List all files in the folder.\n",
    "                if file.endswith(\".wav\"):                          # Only select the .wav files.\n",
    "                    emo_code = file.split(\"-\")[2]                  # Extract the emotion number(0,1,2,etc).\n",
    "\n",
    "                    # USE ONLY OUR 5 TARGET EMOTIONS\n",
    "                    if emo_code in emotion_map:                         # Check condition for the presence of the emotion number in emotion map\n",
    "                        full_path = os.path.join(actor_folder, file)\n",
    "                        label = class_to_idx[emotion_map[emo_code]]     # number conversion.\n",
    "                        self.samples.append((full_path, label))         # add (audioPath,label) into the list created above.\n",
    "\n",
    "    def __len__(self):                                                  # returns total no. of samples.\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):                                         # selects sample at the idx index.\n",
    "        audio_path, label = self.samples[idx]                           # Retrieve file path + label.\n",
    "        img = audio_to_mel_image(audio_path)                            # creating the MEL-Spectrogram.\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer Freezing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T13:22:33.748798Z",
     "iopub.status.busy": "2025-12-08T13:22:33.748552Z",
     "iopub.status.idle": "2025-12-08T13:22:33.761562Z",
     "shell.execute_reply": "2025-12-08T13:22:33.760806Z",
     "shell.execute_reply.started": "2025-12-08T13:22:33.748776Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_resnet50(num_classes):\n",
    "    model = models.resnet50(weights=\"IMAGENET1K_V1\")                    # Creates the ResNet 50 model and loads weights that were trained on the ImageNet dataset\n",
    "                                                                        # This helps in the detection of textures,edges,shapes,etc.\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"layer4\" in name:                                            # allow last block to train\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "    # Replace final FC layer\n",
    "    model.fc = nn.Linear(2048, num_classes)                             # mapping from 2048 till num_classes.\n",
    "    return model                                                        # returns the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T13:22:33.841041Z",
     "iopub.status.busy": "2025-12-08T13:22:33.840630Z",
     "iopub.status.idle": "2025-12-08T13:36:53.724283Z",
     "shell.execute_reply": "2025-12-08T13:36:53.723674Z",
     "shell.execute_reply.started": "2025-12-08T13:22:33.841024Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== STARTING RESNET50 PRETRAINING (5 EMOTIONS) =====\n",
      "\n",
      "Epoch 1/8 | Loss: 101.034 | Accuracy: 39.70%\n",
      "Epoch 2/8 | Loss: 30.036 | Accuracy: 79.98%\n",
      "Epoch 3/8 | Loss: 13.958 | Accuracy: 90.62%\n",
      "Epoch 4/8 | Loss: 10.041 | Accuracy: 93.17%\n",
      "Epoch 5/8 | Loss: 12.944 | Accuracy: 92.94%\n",
      "Epoch 6/8 | Loss: 12.717 | Accuracy: 93.63%\n",
      "Epoch 7/8 | Loss: 15.227 | Accuracy: 91.67%\n",
      "Epoch 8/8 | Loss: 14.257 | Accuracy: 93.17%\n",
      "\n",
      "[SAVED] ravdess_resnet50_5emotion_pretrained.pt\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pretrain_resnet50_ravdess(ravdess_path, epochs=8, batch_size=16):                       # ravdess_path: folder with the RAVDESS audio.\n",
    "\n",
    "                                                                                                      # epochs, batch_size, lr: training hyperparameters.\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "    ])                                                     # transformation before feeding it to the model.\n",
    "\n",
    "    dataset = RAVDESSDataset(ravdess_path, transform)                                                 # scans ravdess_path, filters files by emotion, and will return (mel_image, label) with the transform applied.\n",
    "    \n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)                                 # Wrapping the dataset in a DataLoader that will feed data in mini‑batches.\n",
    "\n",
    "    model = build_resnet50(num_classes=len(classes))                                                  # Builds ResNet‑50 model with a final linear layer sized to no. of emotion classes.\n",
    "\n",
    "    weights = torch.tensor([1.5, 1.0, 1.0, 1.0, 1.5])  # boost neutral & disgust\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights)    # Loss Function.\n",
    "    optimizer = torch.optim.Adam([\n",
    "    {\"params\": model.fc.parameters(), \"lr\": 5e-3},\n",
    "    {\"params\": model.layer4.parameters(), \"lr\": 1e-4},\n",
    "    ])\n",
    "                                                       # Uses the Adam optimizer so that only the final layer will be trained.\n",
    "\n",
    "    print(\"\\n===== STARTING RESNET50 PRETRAINING (5 EMOTIONS) =====\\n\")\n",
    "\n",
    "    for epoch in range(epochs):                                                                       # calculating losses and no. of correct predictions for the looping epochs.\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "\n",
    "        for imgs, labels in loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "        accuracy = correct / len(dataset) * 100\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {total_loss:.3f} | Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"ravdess_resnet50_5emotion_pretrained.pt\")\n",
    "    print(\"\\n[SAVED] ravdess_resnet50_5emotion_pretrained.pt\\n\")\n",
    "\n",
    "    return model\n",
    "    \n",
    "RAVDESS_ROOT = \"/kaggle/input/ravdess-emotional-speech-audio/ravdess\"\n",
    "\n",
    "pretrain_resnet50_ravdess(RAVDESS_ROOT)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-08T13:41:04.033209Z",
     "iopub.status.busy": "2025-12-08T13:41:04.032856Z",
     "iopub.status.idle": "2025-12-08T13:41:06.115168Z",
     "shell.execute_reply": "2025-12-08T13:41:06.114324Z",
     "shell.execute_reply.started": "2025-12-08T13:41:04.033183Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset root: /kaggle/input/fine-tune-emotion-dataset/finetune_dataset\n",
      "Folders: ['Neutral', 'Sad', 'Disgust', 'Happy', 'Anger']\n",
      "Extracting embeddings...\n",
      "Total samples: 17\n",
      "Computing signatures...\n",
      "Signatures ready.\n",
      "Hybrid feature dimension: 2053\n",
      "\n",
      "Training classifier...\n",
      "\n",
      "Epoch 1/60 | Loss=1.6235 | Acc=0.00%\n",
      "Epoch 2/60 | Loss=1.5536 | Acc=29.41%\n",
      "Epoch 3/60 | Loss=1.4716 | Acc=64.71%\n",
      "Epoch 4/60 | Loss=1.3714 | Acc=82.35%\n",
      "Epoch 5/60 | Loss=1.2678 | Acc=82.35%\n",
      "Epoch 6/60 | Loss=1.1529 | Acc=82.35%\n",
      "Epoch 7/60 | Loss=1.0332 | Acc=82.35%\n",
      "Epoch 8/60 | Loss=0.9120 | Acc=100.00%\n",
      "Epoch 9/60 | Loss=0.7895 | Acc=100.00%\n",
      "Epoch 10/60 | Loss=0.6698 | Acc=100.00%\n",
      "Epoch 11/60 | Loss=0.5566 | Acc=100.00%\n",
      "Epoch 12/60 | Loss=0.4519 | Acc=100.00%\n",
      "Epoch 13/60 | Loss=0.3597 | Acc=100.00%\n",
      "Epoch 14/60 | Loss=0.2803 | Acc=100.00%\n",
      "Epoch 15/60 | Loss=0.2138 | Acc=100.00%\n",
      "Epoch 16/60 | Loss=0.1599 | Acc=100.00%\n",
      "Epoch 17/60 | Loss=0.1173 | Acc=100.00%\n",
      "Epoch 18/60 | Loss=0.0847 | Acc=100.00%\n",
      "Epoch 19/60 | Loss=0.0605 | Acc=100.00%\n",
      "Epoch 20/60 | Loss=0.0429 | Acc=100.00%\n",
      "Epoch 21/60 | Loss=0.0304 | Acc=100.00%\n",
      "Epoch 22/60 | Loss=0.0216 | Acc=100.00%\n",
      "Epoch 23/60 | Loss=0.0154 | Acc=100.00%\n",
      "Epoch 24/60 | Loss=0.0112 | Acc=100.00%\n",
      "Epoch 25/60 | Loss=0.0082 | Acc=100.00%\n",
      "Epoch 26/60 | Loss=0.0061 | Acc=100.00%\n",
      "Epoch 27/60 | Loss=0.0046 | Acc=100.00%\n",
      "Epoch 28/60 | Loss=0.0035 | Acc=100.00%\n",
      "Epoch 29/60 | Loss=0.0027 | Acc=100.00%\n",
      "Epoch 30/60 | Loss=0.0021 | Acc=100.00%\n",
      "Epoch 31/60 | Loss=0.0017 | Acc=100.00%\n",
      "Epoch 32/60 | Loss=0.0014 | Acc=100.00%\n",
      "Epoch 33/60 | Loss=0.0012 | Acc=100.00%\n",
      "Epoch 34/60 | Loss=0.0010 | Acc=100.00%\n",
      "Epoch 35/60 | Loss=0.0008 | Acc=100.00%\n",
      "Epoch 36/60 | Loss=0.0007 | Acc=100.00%\n",
      "Epoch 37/60 | Loss=0.0006 | Acc=100.00%\n",
      "Epoch 38/60 | Loss=0.0005 | Acc=100.00%\n",
      "Epoch 39/60 | Loss=0.0005 | Acc=100.00%\n",
      "Epoch 40/60 | Loss=0.0004 | Acc=100.00%\n",
      "Epoch 41/60 | Loss=0.0004 | Acc=100.00%\n",
      "Epoch 42/60 | Loss=0.0003 | Acc=100.00%\n",
      "Epoch 43/60 | Loss=0.0003 | Acc=100.00%\n",
      "Epoch 44/60 | Loss=0.0003 | Acc=100.00%\n",
      "Epoch 45/60 | Loss=0.0003 | Acc=100.00%\n",
      "Epoch 46/60 | Loss=0.0002 | Acc=100.00%\n",
      "Epoch 47/60 | Loss=0.0002 | Acc=100.00%\n",
      "Epoch 48/60 | Loss=0.0002 | Acc=100.00%\n",
      "Epoch 49/60 | Loss=0.0002 | Acc=100.00%\n",
      "Epoch 50/60 | Loss=0.0002 | Acc=100.00%\n",
      "Epoch 51/60 | Loss=0.0002 | Acc=100.00%\n",
      "Epoch 52/60 | Loss=0.0002 | Acc=100.00%\n",
      "Epoch 53/60 | Loss=0.0002 | Acc=100.00%\n",
      "Epoch 54/60 | Loss=0.0002 | Acc=100.00%\n",
      "Epoch 55/60 | Loss=0.0002 | Acc=100.00%\n",
      "Epoch 56/60 | Loss=0.0002 | Acc=100.00%\n",
      "Epoch 57/60 | Loss=0.0002 | Acc=100.00%\n",
      "Epoch 58/60 | Loss=0.0002 | Acc=100.00%\n",
      "Epoch 59/60 | Loss=0.0001 | Acc=100.00%\n",
      "Epoch 60/60 | Loss=0.0001 | Acc=100.00%\n",
      "\n",
      "[SAVED] HYBRID_FINAL_MODEL.pt\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 1. IMPORTS\n",
    "# ============================================\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 2. DATASET PATH (CHANGE ONLY IF NEEDED)\n",
    "# ============================================\n",
    "DATASET_ROOT = \"/kaggle/input/fine-tune-emotion-dataset/finetune_dataset\"\n",
    "\n",
    "emotion_labels = [\"Neutral\", \"Happy\", \"Sad\", \"Anger\", \"Disgust\"]\n",
    "label_to_idx = {e:i for i,e in enumerate(emotion_labels)}\n",
    "\n",
    "print(\"Dataset root:\", DATASET_ROOT)\n",
    "print(\"Folders:\", os.listdir(DATASET_ROOT))\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 3. MEL SPECTROGRAM\n",
    "# ============================================\n",
    "def audio_to_mel_image(path):\n",
    "    y, sr = librosa.load(path, sr=18050)\n",
    "    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=1024,\n",
    "                                         hop_length=256, n_mels=128)\n",
    "    mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "\n",
    "    mel_norm = (mel_db - mel_db.min()) / (mel_db.max() - mel_db.min())\n",
    "    mel_img = (mel_norm * 255).astype(np.uint8)\n",
    "\n",
    "    img = Image.fromarray(mel_img).convert(\"RGB\")\n",
    "    img = img.resize((224,224))\n",
    "    return img\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 4. TRANSFORMS + RESNET50 FEATURE EXTRACTOR\n",
    "# ============================================\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                         std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "class ResNet50_Extractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        net = models.resnet50(weights=\"IMAGENET1K_V1\")\n",
    "        self.feature_extractor = nn.Sequential(*list(net.children())[:-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            z = self.feature_extractor(x)\n",
    "        return z.view(z.size(0), -1)   # 2048-dim\n",
    "\n",
    "\n",
    "resnet = ResNet50_Extractor().eval()\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 5. EXTRACT EMBEDDINGS FROM YOUR DATASET\n",
    "# ============================================\n",
    "all_embeddings = []\n",
    "all_labels = []\n",
    "\n",
    "print(\"Extracting embeddings...\")\n",
    "\n",
    "for emotion in emotion_labels:\n",
    "    folder = os.path.join(DATASET_ROOT, emotion)\n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith(\".wav\"):\n",
    "            path = os.path.join(folder, file)\n",
    "\n",
    "            img = audio_to_mel_image(path)\n",
    "            img = transform(img).unsqueeze(0)\n",
    "\n",
    "            emb = resnet(img)[0].numpy()\n",
    "\n",
    "            all_embeddings.append(emb)\n",
    "            all_labels.append(label_to_idx[emotion])\n",
    "\n",
    "all_embeddings = np.array(all_embeddings)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "print(\"Total samples:\", len(all_embeddings))\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 6. COMPUTE EMOTION SIGNATURES\n",
    "# ============================================\n",
    "print(\"Computing signatures...\")\n",
    "\n",
    "signatures = {}\n",
    "for emotion in emotion_labels:\n",
    "    idxs = np.where(all_labels == label_to_idx[emotion])[0]\n",
    "    signatures[emotion] = np.mean(all_embeddings[idxs], axis=0)\n",
    "\n",
    "print(\"Signatures ready.\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 7. HYBRID FEATURE CREATION\n",
    "# ============================================\n",
    "def hybrid_vector(embedding):\n",
    "    sims = []\n",
    "    for e in emotion_labels:\n",
    "        sim = cosine_similarity([embedding], [signatures[e]])[0][0]\n",
    "        sims.append(sim)\n",
    "    return np.concatenate([embedding, sims])   # 1024 + 5 = 1053\n",
    "\n",
    "\n",
    "X_hybrid = np.array([hybrid_vector(e) for e in all_embeddings])\n",
    "y = all_labels\n",
    "\n",
    "input_dim = X_hybrid.shape[1]\n",
    "print(\"Hybrid feature dimension:\", input_dim)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 8. HYBRID CLASSIFIER\n",
    "# ============================================\n",
    "class HybridClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,5)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "clf = HybridClassifier(input_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(clf.parameters(), lr=1e-3)\n",
    "\n",
    "X_tensor = torch.tensor(X_hybrid, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 9. TRAINING LOOP\n",
    "# ============================================\n",
    "print(\"\\nTraining classifier...\\n\")\n",
    "\n",
    "for epoch in range(60):\n",
    "    optimizer.zero_grad()\n",
    "    out = clf(X_tensor)\n",
    "    loss = criterion(out, y_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    acc = (out.argmax(1) == y_tensor).float().mean().item() * 100\n",
    "    print(f\"Epoch {epoch+1}/60 | Loss={loss:.4f} | Acc={acc:.2f}%\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 10. SAVE FINAL MODEL\n",
    "# ============================================\n",
    "torch.save({\n",
    "    \"model_state\": clf.state_dict(),\n",
    "    \"signatures\": signatures\n",
    "}, \"/kaggle/working/HYBRID_FINAL_MODEL.pt\")\n",
    "\n",
    "print(\"\\n[SAVED] HYBRID_FINAL_MODEL.pt\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 11. INFERENCE FUNCTION\n",
    "# ============================================\n",
    "def predict_emotion(audio_path):\n",
    "    # Step 1: embedding\n",
    "    img = audio_to_mel_image(audio_path)\n",
    "    img = transform(img).unsqueeze(0)\n",
    "    emb = resnet(img)[0].numpy()\n",
    "\n",
    "    # Step 2: similarity scores\n",
    "    sims = []\n",
    "    for e in emotion_labels:\n",
    "        sim = cosine_similarity([emb], [signatures[e]])[0][0]\n",
    "        sims.append(sim)\n",
    "\n",
    "    # Step 3: hybrid vector\n",
    "    vec = np.concatenate([emb, sims])\n",
    "    x = torch.tensor(vec, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    # Step 4: classify\n",
    "    clf.eval()\n",
    "    with torch.no_grad():\n",
    "        out = clf(x)\n",
    "        idx = out.argmax(1).item()\n",
    "        confidence = torch.softmax(out,1).max().item()\n",
    "\n",
    "    return emotion_labels[idx], confidence\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 12. TEST PREDICTION\n",
    "# ============================================\n",
    "# Example:\n",
    "#test_file = \"/kaggle/input/fine-tune-emotion-dataset/happy/sample1.wav\"\n",
    "#motion, conf = predict_emotion(test_file)\n",
    "#print(\"Predicted:\", emotion, \"| Confidence:\", round(conf*100,2), \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backend Integration\n",
    "\n",
    "Connect the notebook to the running FastAPI backend at `http://127.0.0.1:8000/classify` for real-time emotion detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# ============================================\n",
    "# BACKEND API CLIENT\n",
    "# ============================================\n",
    "def classify_via_backend(audio_path, backend_url=\"http://127.0.0.1:8000\"):\n",
    "    \"\"\"\n",
    "    Send audio file to the backend /classify endpoint.\n",
    "    Returns: {\"state\": emotion, \"accuracy\": confidence}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(audio_path, 'rb') as f:\n",
    "            files = {'audio': (audio_path, f, 'audio/wav')}\n",
    "            data = {'message': 'notebook-test'}\n",
    "            \n",
    "            r = requests.post(f\"{backend_url}/classify\", files=files, data=data, timeout=10)\n",
    "            \n",
    "            if r.status_code == 200:\n",
    "                return r.json()\n",
    "            else:\n",
    "                print(f\"[ERROR] Backend returned {r.status_code}: {r.text}\")\n",
    "                return None\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"[ERROR] Cannot connect to backend at\", backend_url)\n",
    "        print(\"Make sure backend is running: python -m uvicorn main:app --port 8000\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# TEST BACKEND CONNECTION\n",
    "# ============================================\n",
    "print(\"Testing backend connection...\")\n",
    "backend_result = classify_via_backend(r\"D:\\Maitri\\tools\\test_sine.wav\")\n",
    "\n",
    "if backend_result:\n",
    "    print(f\"✓ Backend OK\")\n",
    "    print(f\"  Emotion: {backend_result.get('state')}\")\n",
    "    print(f\"  Confidence: {backend_result.get('accuracy')}\")\n",
    "else:\n",
    "    print(f\"✗ Backend connection failed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: Local Model vs Backend\n",
    "\n",
    "Compare the results from the local trained classifier with the backend inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# COMPARE LOCAL vs BACKEND\n",
    "# ============================================\n",
    "test_audio = r\"D:\\Maitri\\tools\\test_sine.wav\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INFERENCE COMPARISON: Local Model vs Backend\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Local model (from notebook)\n",
    "print(\"\\n[LOCAL MODEL]\")\n",
    "local_emotion, local_conf = predict_emotion(test_audio)\n",
    "print(f\"  Emotion: {local_emotion}\")\n",
    "print(f\"  Confidence: {round(local_conf * 100, 2)}%\")\n",
    "\n",
    "# Backend server\n",
    "print(\"\\n[BACKEND SERVER]\")\n",
    "backend_result = classify_via_backend(test_audio)\n",
    "if backend_result:\n",
    "    backend_emotion = backend_result.get('state', 'Unknown')\n",
    "    backend_conf = backend_result.get('accuracy', 0.0)\n",
    "    print(f\"  Emotion: {backend_emotion}\")\n",
    "    print(f\"  Confidence: {round(backend_conf * 100, 2)}%\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n[SUMMARY]\")\n",
    "    match = \"✓ MATCH\" if local_emotion.lower() == backend_emotion.lower() else \"✗ MISMATCH\"\n",
    "    print(f\"  Results: {match}\")\n",
    "else:\n",
    "    print(\"  [Connection failed]\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8947281,
     "sourceId": 14057112,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
