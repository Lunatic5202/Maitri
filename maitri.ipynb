{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14057112,"sourceType":"datasetVersion","datasetId":8947281}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Importing the necessary modules\nimport os\nimport numpy as np\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nfrom torchvision import models, transforms\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-08T13:22:33.684165Z","iopub.execute_input":"2025-12-08T13:22:33.684901Z","iopub.status.idle":"2025-12-08T13:22:33.688981Z","shell.execute_reply.started":"2025-12-08T13:22:33.684878Z","shell.execute_reply":"2025-12-08T13:22:33.688429Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"Emotions used:\n\n01 -> neutral\n\n02 -> happy\n\n03 -> sad\n\n04 -> angry\n\n05 -> disgust","metadata":{}},{"cell_type":"code","source":"import os\n\nprint(\"CHECK 1: Does the folder exist?\")\nprint(os.path.exists(\"/kaggle/input/ravdess-emotional-speech-audio/ravdess\"))\n\nprint(\"\\nCHECK 2: List contents of /kaggle/input/\")\nprint(os.listdir(\"/kaggle/input\"))\n\nprint(\"\\nCHECK 3: List contents of ravdess-emotional-speech-audio folder\")\nprint(os.listdir(\"/kaggle/input/ravdess-emotional-speech-audio\"))\n\nprint(\"\\nCHECK 4: List contents of ravdess folder\")\nprint(os.listdir(\"/kaggle/input/ravdess-emotional-speech-audio/ravdess\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T13:22:33.690585Z","iopub.execute_input":"2025-12-08T13:22:33.691099Z","iopub.status.idle":"2025-12-08T13:22:33.703928Z","shell.execute_reply.started":"2025-12-08T13:22:33.691080Z","shell.execute_reply":"2025-12-08T13:22:33.703318Z"}},"outputs":[{"name":"stdout","text":"CHECK 1: Does the folder exist?\nTrue\n\nCHECK 2: List contents of /kaggle/input/\n['ravdess-emotional-speech-audio', 'fine-tune-emotion-dataset']\n\nCHECK 3: List contents of ravdess-emotional-speech-audio folder\n['ravdess']\n\nCHECK 4: List contents of ravdess folder\n['Actor_02', 'Actor_17', 'Actor_05', 'Actor_16', 'Actor_21', 'Actor_01', 'Actor_11', 'Actor_20', 'Actor_08', 'Actor_15', 'Actor_06', 'Actor_12', 'Actor_23', 'Actor_24', 'Actor_22', 'Actor_04', 'Actor_19', 'Actor_10', 'Actor_09', 'Actor_14', 'Actor_03', 'Actor_13', 'Actor_18', 'Actor_07']\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"emotion_map = {\n    \"01\": \"neutral\",\n    \"03\": \"happy\",\n    \"04\": \"sad\",\n    \"05\": \"angry\",\n    \"07\": \"disgust\"\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T13:22:33.704574Z","iopub.execute_input":"2025-12-08T13:22:33.704756Z","iopub.status.idle":"2025-12-08T13:22:33.711667Z","shell.execute_reply.started":"2025-12-08T13:22:33.704742Z","shell.execute_reply":"2025-12-08T13:22:33.711008Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"Extracting all the emotion names as ML cannot work with words.","metadata":{}},{"cell_type":"code","source":"classes = list(emotion_map.values())\nclass_to_idx = {c: i for i, c in enumerate(classes)}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T13:22:33.712430Z","iopub.execute_input":"2025-12-08T13:22:33.712747Z","iopub.status.idle":"2025-12-08T13:22:33.722319Z","shell.execute_reply.started":"2025-12-08T13:22:33.712718Z","shell.execute_reply":"2025-12-08T13:22:33.721618Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"Audio to Mel Spectrogram","metadata":{}},{"cell_type":"code","source":"def audio_to_mel_image(path):\n    y, sr = librosa.load(path, sr=18050) # sr indicates sampling rate, y indicates audio waveform (1D array) , Fs = 18.05 KHz.\n\n    mel = librosa.feature.melspectrogram(\n        y=y, sr=sr, n_fft=1024, hop_length=256, n_mels=128 ) # window_size = 1024.....converts raw audio into a visual time-freq plot.\n    mel_db = librosa.power_to_db(mel, ref=np.max) # convert signal to dB unit for better representation and stability.\n\n    mel_norm = (mel_db - mel_db.min()) / (mel_db.max() - mel_db.min()) # normalise the range between 0 and 1 for input simplicity.\n    mel_img = (mel_norm * 255).astype(np.uint8) # convert to pixels(0-255).\n\n    img = Image.fromarray(mel_img).convert(\"RGB\") # ResNet50 needs RGB images.\n    img = img.resize((224, 224)) # input requirement.\n    return img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T13:22:33.723558Z","iopub.execute_input":"2025-12-08T13:22:33.724649Z","iopub.status.idle":"2025-12-08T13:22:33.733939Z","shell.execute_reply.started":"2025-12-08T13:22:33.724632Z","shell.execute_reply":"2025-12-08T13:22:33.733390Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"Old labels → used for the signature model\n\nNew labels → used for the ResNet50 model","metadata":{}},{"cell_type":"code","source":"# During initialization, RAVDESSDataset scans the RAVDESS folder, picks only .wav files whose emotion code is ni target set, and stores (audio_path, label_index) pairs in self.samples.\n# When the DataLoader asks for item i, __getitem__ looks up the path and label, and then calls audio_to_mel_image(audio_path) to actually build the input for the model.\n\n    \nclass RAVDESSDataset(Dataset):                                     # creating a custom PyTorch Dataset class that the DataLoader can use.\n    \n    def __init__(self, root_dir, transform=None):                  # root_dir → path to RAVDESS audio folder\n                                                                   # transform → any optional image transforms (resize, normalize, etc.)\n        \n        self.samples = []                                          # stores all (audioPath,labels) in a single list.\n        self.transform = transform\n\n        for actor in os.listdir(root_dir):                         # List all items in the main RAVDESS folder.\n                                                                   # Each \"Actor_xy\" is one folder.\n            \n            actor_folder = os.path.join(root_dir, actor)           # Create the full path......\n            if not os.path.isdir(actor_folder):                    # skip anything that is not a folder.\n                continue\n\n            for file in os.listdir(actor_folder):                  # List all files in the folder.\n                if file.endswith(\".wav\"):                          # Only select the .wav files.\n                    emo_code = file.split(\"-\")[2]                  # Extract the emotion number(0,1,2,etc).\n\n                    # USE ONLY OUR 5 TARGET EMOTIONS\n                    if emo_code in emotion_map:                         # Check condition for the presence of the emotion number in emotion map\n                        full_path = os.path.join(actor_folder, file)\n                        label = class_to_idx[emotion_map[emo_code]]     # number conversion.\n                        self.samples.append((full_path, label))         # add (audioPath,label) into the list created above.\n\n    def __len__(self):                                                  # returns total no. of samples.\n        return len(self.samples)\n\n    def __getitem__(self, idx):                                         # selects sample at the idx index.\n        audio_path, label = self.samples[idx]                           # Retrieve file path + label.\n        img = audio_to_mel_image(audio_path)                            # creating the MEL-Spectrogram.\n\n        if self.transform:\n            img = self.transform(img)\n\n        return img, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T13:22:33.734702Z","iopub.execute_input":"2025-12-08T13:22:33.735464Z","iopub.status.idle":"2025-12-08T13:22:33.747830Z","shell.execute_reply.started":"2025-12-08T13:22:33.735439Z","shell.execute_reply":"2025-12-08T13:22:33.747169Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"Layer Freezing\n","metadata":{}},{"cell_type":"code","source":"def build_resnet50(num_classes):\n    model = models.resnet50(weights=\"IMAGENET1K_V1\")                    # Creates the ResNet 50 model and loads weights that were trained on the ImageNet dataset\n                                                                        # This helps in the detection of textures,edges,shapes,etc.\n\n    for name, param in model.named_parameters():\n        if \"layer4\" in name:                                            # allow last block to train\n            param.requires_grad = True\n        else:\n            param.requires_grad = False\n\n\n    # Replace final FC layer\n    model.fc = nn.Linear(2048, num_classes)                             # mapping from 2048 till num_classes.\n    return model                                                        # returns the model.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T13:22:33.748552Z","iopub.execute_input":"2025-12-08T13:22:33.748798Z","iopub.status.idle":"2025-12-08T13:22:33.761562Z","shell.execute_reply.started":"2025-12-08T13:22:33.748776Z","shell.execute_reply":"2025-12-08T13:22:33.760806Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"Pre-Training","metadata":{}},{"cell_type":"code","source":"def pretrain_resnet50_ravdess(ravdess_path, epochs=8, batch_size=16):                       # ravdess_path: folder with the RAVDESS audio.\n\n                                                                                                      # epochs, batch_size, lr: training hyperparameters.\n\n    transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n    ])                                                     # transformation before feeding it to the model.\n\n    dataset = RAVDESSDataset(ravdess_path, transform)                                                 # scans ravdess_path, filters files by emotion, and will return (mel_image, label) with the transform applied.\n    \n    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)                                 # Wrapping the dataset in a DataLoader that will feed data in mini‑batches.\n\n    model = build_resnet50(num_classes=len(classes))                                                  # Builds ResNet‑50 model with a final linear layer sized to no. of emotion classes.\n\n    weights = torch.tensor([1.5, 1.0, 1.0, 1.0, 1.5])  # boost neutral & disgust\n    criterion = nn.CrossEntropyLoss(weight=weights)    # Loss Function.\n    optimizer = torch.optim.Adam([\n    {\"params\": model.fc.parameters(), \"lr\": 5e-3},\n    {\"params\": model.layer4.parameters(), \"lr\": 1e-4},\n    ])\n                                                       # Uses the Adam optimizer so that only the final layer will be trained.\n\n    print(\"\\n===== STARTING RESNET50 PRETRAINING (5 EMOTIONS) =====\\n\")\n\n    for epoch in range(epochs):                                                                       # calculating losses and no. of correct predictions for the looping epochs.\n        total_loss = 0\n        correct = 0\n\n        for imgs, labels in loader:\n            optimizer.zero_grad()\n            outputs = model(imgs)\n            loss = criterion(outputs, labels)\n\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n            correct += (outputs.argmax(dim=1) == labels).sum().item()\n\n        accuracy = correct / len(dataset) * 100\n        print(f\"Epoch {epoch+1}/{epochs} | Loss: {total_loss:.3f} | Accuracy: {accuracy:.2f}%\")\n\n    torch.save(model.state_dict(), \"ravdess_resnet50_5emotion_pretrained.pt\")\n    print(\"\\n[SAVED] ravdess_resnet50_5emotion_pretrained.pt\\n\")\n\n    return model\n    \nRAVDESS_ROOT = \"/kaggle/input/ravdess-emotional-speech-audio/ravdess\"\n\npretrain_resnet50_ravdess(RAVDESS_ROOT)    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T13:22:33.840630Z","iopub.execute_input":"2025-12-08T13:22:33.841041Z","iopub.status.idle":"2025-12-08T13:36:53.724283Z","shell.execute_reply.started":"2025-12-08T13:22:33.841024Z","shell.execute_reply":"2025-12-08T13:36:53.723674Z"}},"outputs":[{"name":"stdout","text":"\n===== STARTING RESNET50 PRETRAINING (5 EMOTIONS) =====\n\nEpoch 1/8 | Loss: 101.034 | Accuracy: 39.70%\nEpoch 2/8 | Loss: 30.036 | Accuracy: 79.98%\nEpoch 3/8 | Loss: 13.958 | Accuracy: 90.62%\nEpoch 4/8 | Loss: 10.041 | Accuracy: 93.17%\nEpoch 5/8 | Loss: 12.944 | Accuracy: 92.94%\nEpoch 6/8 | Loss: 12.717 | Accuracy: 93.63%\nEpoch 7/8 | Loss: 15.227 | Accuracy: 91.67%\nEpoch 8/8 | Loss: 14.257 | Accuracy: 93.17%\n\n[SAVED] ravdess_resnet50_5emotion_pretrained.pt\n\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (4): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (5): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=2048, out_features=5, bias=True)\n)"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"Fine Tuning\n","metadata":{}},{"cell_type":"code","source":"# ============================================\n# 1. IMPORTS\n# ============================================\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n\n# ============================================\n# 2. DATASET PATH (CHANGE ONLY IF NEEDED)\n# ============================================\nDATASET_ROOT = \"/kaggle/input/fine-tune-emotion-dataset/finetune_dataset\"\n\nemotion_labels = [\"Neutral\", \"Happy\", \"Sad\", \"Anger\", \"Disgust\"]\nlabel_to_idx = {e:i for i,e in enumerate(emotion_labels)}\n\nprint(\"Dataset root:\", DATASET_ROOT)\nprint(\"Folders:\", os.listdir(DATASET_ROOT))\n\n\n# ============================================\n# 3. MEL SPECTROGRAM\n# ============================================\ndef audio_to_mel_image(path):\n    y, sr = librosa.load(path, sr=18050)\n    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=1024,\n                                         hop_length=256, n_mels=128)\n    mel_db = librosa.power_to_db(mel, ref=np.max)\n\n    mel_norm = (mel_db - mel_db.min()) / (mel_db.max() - mel_db.min())\n    mel_img = (mel_norm * 255).astype(np.uint8)\n\n    img = Image.fromarray(mel_img).convert(\"RGB\")\n    img = img.resize((224,224))\n    return img\n\n\n# ============================================\n# 4. TRANSFORMS + RESNET50 FEATURE EXTRACTOR\n# ============================================\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485,0.456,0.406],\n                         std=[0.229,0.224,0.225])\n])\n\nclass ResNet50_Extractor(nn.Module):\n    def __init__(self):\n        super().__init__()\n        net = models.resnet50(weights=\"IMAGENET1K_V1\")\n        self.feature_extractor = nn.Sequential(*list(net.children())[:-1])\n\n    def forward(self, x):\n        with torch.no_grad():\n            z = self.feature_extractor(x)\n        return z.view(z.size(0), -1)   # 2048-dim\n\n\nresnet = ResNet50_Extractor().eval()\n\n\n# ============================================\n# 5. EXTRACT EMBEDDINGS FROM YOUR DATASET\n# ============================================\nall_embeddings = []\nall_labels = []\n\nprint(\"Extracting embeddings...\")\n\nfor emotion in emotion_labels:\n    folder = os.path.join(DATASET_ROOT, emotion)\n    for file in os.listdir(folder):\n        if file.endswith(\".wav\"):\n            path = os.path.join(folder, file)\n\n            img = audio_to_mel_image(path)\n            img = transform(img).unsqueeze(0)\n\n            emb = resnet(img)[0].numpy()\n\n            all_embeddings.append(emb)\n            all_labels.append(label_to_idx[emotion])\n\nall_embeddings = np.array(all_embeddings)\nall_labels = np.array(all_labels)\n\nprint(\"Total samples:\", len(all_embeddings))\n\n\n# ============================================\n# 6. COMPUTE EMOTION SIGNATURES\n# ============================================\nprint(\"Computing signatures...\")\n\nsignatures = {}\nfor emotion in emotion_labels:\n    idxs = np.where(all_labels == label_to_idx[emotion])[0]\n    signatures[emotion] = np.mean(all_embeddings[idxs], axis=0)\n\nprint(\"Signatures ready.\")\n\n\n# ============================================\n# 7. HYBRID FEATURE CREATION\n# ============================================\ndef hybrid_vector(embedding):\n    sims = []\n    for e in emotion_labels:\n        sim = cosine_similarity([embedding], [signatures[e]])[0][0]\n        sims.append(sim)\n    return np.concatenate([embedding, sims])   # 1024 + 5 = 1053\n\n\nX_hybrid = np.array([hybrid_vector(e) for e in all_embeddings])\ny = all_labels\n\ninput_dim = X_hybrid.shape[1]\nprint(\"Hybrid feature dimension:\", input_dim)\n\n\n# ============================================\n# 8. HYBRID CLASSIFIER\n# ============================================\nclass HybridClassifier(nn.Module):\n    def __init__(self, input_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256,128),\n            nn.ReLU(),\n            nn.Linear(128,5)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclf = HybridClassifier(input_dim)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(clf.parameters(), lr=1e-3)\n\nX_tensor = torch.tensor(X_hybrid, dtype=torch.float32)\ny_tensor = torch.tensor(y, dtype=torch.long)\n\n\n# ============================================\n# 9. TRAINING LOOP\n# ============================================\nprint(\"\\nTraining classifier...\\n\")\n\nfor epoch in range(60):\n    optimizer.zero_grad()\n    out = clf(X_tensor)\n    loss = criterion(out, y_tensor)\n    loss.backward()\n    optimizer.step()\n\n    acc = (out.argmax(1) == y_tensor).float().mean().item() * 100\n    print(f\"Epoch {epoch+1}/60 | Loss={loss:.4f} | Acc={acc:.2f}%\")\n\n\n# ============================================\n# 10. SAVE FINAL MODEL\n# ============================================\ntorch.save({\n    \"model_state\": clf.state_dict(),\n    \"signatures\": signatures\n}, \"/kaggle/working/HYBRID_FINAL_MODEL.pt\")\n\nprint(\"\\n[SAVED] HYBRID_FINAL_MODEL.pt\")\n\n\n# ============================================\n# 11. INFERENCE FUNCTION\n# ============================================\ndef predict_emotion(audio_path):\n    # Step 1: embedding\n    img = audio_to_mel_image(audio_path)\n    img = transform(img).unsqueeze(0)\n    emb = resnet(img)[0].numpy()\n\n    # Step 2: similarity scores\n    sims = []\n    for e in emotion_labels:\n        sim = cosine_similarity([emb], [signatures[e]])[0][0]\n        sims.append(sim)\n\n    # Step 3: hybrid vector\n    vec = np.concatenate([emb, sims])\n    x = torch.tensor(vec, dtype=torch.float32).unsqueeze(0)\n\n    # Step 4: classify\n    clf.eval()\n    with torch.no_grad():\n        out = clf(x)\n        idx = out.argmax(1).item()\n        confidence = torch.softmax(out,1).max().item()\n\n    return emotion_labels[idx], confidence\n\n\n# ============================================\n# 12. TEST PREDICTION\n# ============================================\n# Example:\n#test_file = \"/kaggle/input/fine-tune-emotion-dataset/happy/sample1.wav\"\n#motion, conf = predict_emotion(test_file)\n#print(\"Predicted:\", emotion, \"| Confidence:\", round(conf*100,2), \"%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T13:41:04.032856Z","iopub.execute_input":"2025-12-08T13:41:04.033209Z","iopub.status.idle":"2025-12-08T13:41:06.115168Z","shell.execute_reply.started":"2025-12-08T13:41:04.033183Z","shell.execute_reply":"2025-12-08T13:41:06.114324Z"}},"outputs":[{"name":"stdout","text":"Dataset root: /kaggle/input/fine-tune-emotion-dataset/finetune_dataset\nFolders: ['Neutral', 'Sad', 'Disgust', 'Happy', 'Anger']\nExtracting embeddings...\nTotal samples: 17\nComputing signatures...\nSignatures ready.\nHybrid feature dimension: 2053\n\nTraining classifier...\n\nEpoch 1/60 | Loss=1.6235 | Acc=0.00%\nEpoch 2/60 | Loss=1.5536 | Acc=29.41%\nEpoch 3/60 | Loss=1.4716 | Acc=64.71%\nEpoch 4/60 | Loss=1.3714 | Acc=82.35%\nEpoch 5/60 | Loss=1.2678 | Acc=82.35%\nEpoch 6/60 | Loss=1.1529 | Acc=82.35%\nEpoch 7/60 | Loss=1.0332 | Acc=82.35%\nEpoch 8/60 | Loss=0.9120 | Acc=100.00%\nEpoch 9/60 | Loss=0.7895 | Acc=100.00%\nEpoch 10/60 | Loss=0.6698 | Acc=100.00%\nEpoch 11/60 | Loss=0.5566 | Acc=100.00%\nEpoch 12/60 | Loss=0.4519 | Acc=100.00%\nEpoch 13/60 | Loss=0.3597 | Acc=100.00%\nEpoch 14/60 | Loss=0.2803 | Acc=100.00%\nEpoch 15/60 | Loss=0.2138 | Acc=100.00%\nEpoch 16/60 | Loss=0.1599 | Acc=100.00%\nEpoch 17/60 | Loss=0.1173 | Acc=100.00%\nEpoch 18/60 | Loss=0.0847 | Acc=100.00%\nEpoch 19/60 | Loss=0.0605 | Acc=100.00%\nEpoch 20/60 | Loss=0.0429 | Acc=100.00%\nEpoch 21/60 | Loss=0.0304 | Acc=100.00%\nEpoch 22/60 | Loss=0.0216 | Acc=100.00%\nEpoch 23/60 | Loss=0.0154 | Acc=100.00%\nEpoch 24/60 | Loss=0.0112 | Acc=100.00%\nEpoch 25/60 | Loss=0.0082 | Acc=100.00%\nEpoch 26/60 | Loss=0.0061 | Acc=100.00%\nEpoch 27/60 | Loss=0.0046 | Acc=100.00%\nEpoch 28/60 | Loss=0.0035 | Acc=100.00%\nEpoch 29/60 | Loss=0.0027 | Acc=100.00%\nEpoch 30/60 | Loss=0.0021 | Acc=100.00%\nEpoch 31/60 | Loss=0.0017 | Acc=100.00%\nEpoch 32/60 | Loss=0.0014 | Acc=100.00%\nEpoch 33/60 | Loss=0.0012 | Acc=100.00%\nEpoch 34/60 | Loss=0.0010 | Acc=100.00%\nEpoch 35/60 | Loss=0.0008 | Acc=100.00%\nEpoch 36/60 | Loss=0.0007 | Acc=100.00%\nEpoch 37/60 | Loss=0.0006 | Acc=100.00%\nEpoch 38/60 | Loss=0.0005 | Acc=100.00%\nEpoch 39/60 | Loss=0.0005 | Acc=100.00%\nEpoch 40/60 | Loss=0.0004 | Acc=100.00%\nEpoch 41/60 | Loss=0.0004 | Acc=100.00%\nEpoch 42/60 | Loss=0.0003 | Acc=100.00%\nEpoch 43/60 | Loss=0.0003 | Acc=100.00%\nEpoch 44/60 | Loss=0.0003 | Acc=100.00%\nEpoch 45/60 | Loss=0.0003 | Acc=100.00%\nEpoch 46/60 | Loss=0.0002 | Acc=100.00%\nEpoch 47/60 | Loss=0.0002 | Acc=100.00%\nEpoch 48/60 | Loss=0.0002 | Acc=100.00%\nEpoch 49/60 | Loss=0.0002 | Acc=100.00%\nEpoch 50/60 | Loss=0.0002 | Acc=100.00%\nEpoch 51/60 | Loss=0.0002 | Acc=100.00%\nEpoch 52/60 | Loss=0.0002 | Acc=100.00%\nEpoch 53/60 | Loss=0.0002 | Acc=100.00%\nEpoch 54/60 | Loss=0.0002 | Acc=100.00%\nEpoch 55/60 | Loss=0.0002 | Acc=100.00%\nEpoch 56/60 | Loss=0.0002 | Acc=100.00%\nEpoch 57/60 | Loss=0.0002 | Acc=100.00%\nEpoch 58/60 | Loss=0.0002 | Acc=100.00%\nEpoch 59/60 | Loss=0.0001 | Acc=100.00%\nEpoch 60/60 | Loss=0.0001 | Acc=100.00%\n\n[SAVED] HYBRID_FINAL_MODEL.pt\n","output_type":"stream"}],"execution_count":20}]}